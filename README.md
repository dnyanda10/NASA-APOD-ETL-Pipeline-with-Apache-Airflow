## ğŸ“Œ Project Overview
This project implements an **ETL pipeline** using **Apache Airflow** to fetch NASAâ€™s Astronomy Picture of the Day (APOD) data via API, transform it, and store it in a **PostgreSQL database** hosted on **AWS RDS**.  
The pipeline is fully **containerized with Docker** and deployed to **Astronomer Cloud** for automated production scheduling.

**Use Case:** Automates ingestion of NASA APOD data for analytical purposes, reporting, and visualizations.


## ğŸ› ï¸ Tech Stack
- **Apache Airflow** (via Astronomer CLI)
- **Python**
- **PostgreSQL** (AWS RDS)
- **NASA API**
- **Docker**
- **Astronomer Cloud**
  

  ## ğŸ–¼ï¸ Sample Data
The following screenshot shows a sample of the data ingested into the `apod_data` table in PostgreSQL:
![Sample Data](Images/sample_data_airflow_project.jpg)


## ğŸ“‚ Project Structure
```text
.
â”œâ”€â”€ .astro/                    # Astronomer configuration files
â”œâ”€â”€ Architecture Diagram/      # Contains architecture diagrams for documentation
â”œâ”€â”€ dags/                      # Airflow DAG scripts
â”œâ”€â”€ tests/                     # Unit tests (if applicable)
â”œâ”€â”€ Images/                     # Sample screenshots and data visuals
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env                        # Local environment variables (ignored in Git)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ airflow_settings.yaml       # Airflow connection setup
â”œâ”€â”€ docker-compose.yml          # Local Airflow environment setup
â”œâ”€â”€ Dockerfile                  # Custom image for Astronomer deployment
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt            # Python dependencies

```

## âš™ï¸ Airflow Connections Used
The following Airflow connections are configured for this project:

1- nasa_api
   - Type: HTTP
   - Host: https://api.nasa.gov
   - Extra: API Key stored as an environment variable (NASA_API_KEY)

2- my_postgres_connection
   - Type: Postgres
   - Host: AWS RDS endpoint
   - Login: RDS username
   - Password: RDS password
   - Database: Target PostgreSQL DB


## ğŸ“œ DAG Workflow

 - Create Table â€“ Ensures the apod_data table exists in Postgres.
 - Extract Data â€“ Calls NASA APOD API using HttpOperator.
 - Transform Data â€“ Filters and structures JSON response using TaskFlow API.
 - Load Data â€“ Inserts transformed data into Postgres using PostgresHook.


## ğŸš€ Deployment to Production with Astronomer.io
This project is deployed to Astronomer Cloud for production orchestration.

## Steps:
1- Set up Astronomer Cloud Workspace
    - Create a new workspace in Astronomer.io and link your GitHub repo.

2- Configure Production Environment
    - Create .env file with production credentials:

     NASA_API_KEY=your_prod_api_key
     POSTGRES_USER=your_prod_user
     POSTGRES_PASSWORD=your_prod_password
     POSTGRES_HOST=your_rds_endpoint.amazonaws.com
     POSTGRES_DB=your_prod_db

3- Set Airflow Connections in Astronomer UI
     - nasa_api â†’ HTTP with host https://api.nasa.gov
     - my_postgres_connection â†’ Postgres with AWS RDS hostname

4- Deploy to Astronomer
     - astro deploy <deployment-id>
     
5- Monitor & Scale
     - Use Astronomer Cloud to monitor DAG runs and adjust resources.
     
     
     
## ğŸ“… DAG Schedule
- DAG runs daily at 10:00 UTC to retrieve the latest APOD.
- Note: Schedule can be adjusted in the DAG's schedule_interval.
  
  

## ğŸ† Key Highlights
- Fully containerized Airflow setup
- Automated daily API data ingestion
- AWS RDS integration for persistence
- Production-ready deployment via Astronomer.io
  
  

## ğŸ¯ Learning Outcomes
- How to orchestrate ETL pipelines in Airflow
- Using Airflow Hooks & Operators for API and DB operations
- Managing connections securely in Airflow
- Dockerizing Airflow with PostgreSQL setup






